# 统计机器学习05：支持向量机SVM和Kernel方法



支持向量的引入
--------------

对于一个分类问题，我们如果可以找到一个向量$\omega$，使得对于数据集D中的任何样本x，如果x是正例(用+1表示)就有$\omega^Tx> 0$，如果x是反例(用-1表示)就有$\omega^Tx<0$，那么我们就可以很好地对数据集进行分类，判断依据就是$\omega^Tx$

我们也可以这样来考虑，将数据集D中的每个样本x投射到d维的空间上，如果我们可以找到一个d维空间里的超平面(hyperplane)$\omega^Tx+b=0$，将这个空间划分成两个部分，其中一个部分里的样本x全是正例，另一个空间里的样本x全是反例，那么这个数据集的分类问题就解决了，但是实际情况并不会这么好，对于d维空间里的点x，其到超平面的距离可以表示为：

$$
r=\frac{|\omega^Tx+b|}{||\omega||}
$$

又为了能正确地进行分类，对于训练集中的数据，需要有：

$$
\begin{aligned}
    \left\{\begin{aligned}
        \omega^Tx_i+b \le -1, y_i=-1 \\
        \omega^Tx_i+b \ge +1, y_i=+1 \\
        \end{aligned}\right.\end{aligned}
$$

并且存在一些样本使得上面的等号成立，我们就称这些使得等号取到的点称为支持向量(support machine)，每个支持向量到超平面的距离是：
$$
r=\frac{1}{||\omega||}
$$
则超平面两侧的两个支持向量到超平面的距离之和就是：
$$
\gamma=\frac{2}{||\omega||}
$$
这个量也被称为间隔(margin)，为了使得分类效果尽可能地好，我们应该让这个间隔尽可能地大，因此我们的目标可以转化为求$||\omega||^2$的最小值，即 

$$
\begin{aligned}
        & \min_{\omega,b}\frac{1}{2}||\omega||^2\\
        & s.t. y_i(\omega^Tx_i+b)\ge 1
    \end{aligned}
$$

这个优化问题实际上就是支持向量机的基本形式。



松弛变量slack variable
----------------------

在SVM问题求解中我们一般选择整个数据集中最小的一组间隔作为整个数据集的间隔，而我们优化的目标就是让这个最小间隔最大化。但是现实往往没有这么美好，数据的分布不会像我们预想的那么完美，因此我们可以引入松弛变量(slack variable)，给间隔一个**上下浮动的空间**，即可以将问题转化成：

$$
\begin{aligned}
        & \min\limits_{\omega,b} \frac{1}{2}||\omega||^2+C\sum\limits_{i=1}^n\xi_i\\
        & y_i(\omega^Tx_i+b)\ge 1-\xi_i\cap \xi_i\ge 0
    \end{aligned}
$$

我们可以将上面的约束条件转化为： 

$$
\begin{aligned}
    \xi_i\ge 1- y_i(\omega^Tx_i+b)\cap \xi_i\ge 0\\
    \Rightarrow \xi_i = \max \lbrace 1- y_i(\omega^Tx_i+b),0\rbrace\end{aligned}
$$

则优化的目标可以等价于：

$$
\min\limits_{\omega,b}(\sum\limits_{i=0}^{n}\max(1-y(\omega^Tx_i+b),0)+\frac{1}{2C}||\omega||^2)
$$

在求解SVM的过程中，可以将这个式子的**前半部分作为loss function，后半部分作为regularizer**

线性模型的统一性
----------------

我们可以把上面的损失函数记作： $$l(f)=\max [1-yf, 0]$$，我们称这种类型的损失函数为hinge loss(铰链损失函数)，因为其函数图像是一个类似于折线的形状。则我们的优化目标可以写成：

$$
\min\lbrace\sum_{i=0}\limits^{n}l(f)+\lambda R(f)\rbrace
$$

其中R(f)是正则项。事实上前面的所有线性模型，包括线性回归和逻辑回归的优化目标都可以写成上面的形式，区别在于loss函数的选择不同，线性回归选择的loss是Square loss，而逻辑回归选择了Logistic loss，这几种loss函数的图像也各有特点：

-   0-1 loss只有0和1两种值，在优化目标化简的时候特别方便

-   Square loss的波动幅度比较大，更能反映出极端情况下的损失

-   Logistic loss的变动比较平缓但是永远存在

-   Hinge loss在一定情况下会变成0，而在非0的时候比较平缓

核Kernel
--------

### 核方法 Kernel Method

支持向量机问题中，我们要求解的目标就是一个超平面$y=\omega^Tx+b$，用这个超平面来对数据集D进行线性的分割，这时候就出现了一个问题，如果数据不能被线性分割该怎么办？事实上我们之前在SVM以及其他的线性模型中都默认了数据集D是线性可分的，如果实际情况中碰到的并非这么理想，我们就应该采取一定的办法使得现有的数据变得线性可分。

核方法就可以解决这个问题，核方法通过将原始空间映射到一个更高维的特征空间中，使得数据集D中的样本x线性可分，而此时样本x也从一个d维向量映射成了一个更高维度的向量(可以假定高维特征空间的维度是$\chi$)，用$\phi(x)$，则我们需要求解的问题变成了：

$$
y=\omega^T\phi(x)+b
$$


### 核函数 Kernel Function

而在使用核方法的时候经常会需要计算两个向量的内积，由于特征空间的维度可能很高甚至是无穷维，因此我们可以用一个求解简单的核函数来代替内积，使得两个向量在特征空间的内积等于其原本的形式通过函数$K(x_i,x_j)$计算出来的结果，这就是kernel trick

根据向量内积的性质我们可以推测出，和函数一定是对称的，并且运算的结果需要时非负数，即有：
$$K(x_i,x_j)=K(x_j,x_i)\ge 0$$而 对于$|D|=m$，$K(x_i,x_j)$可以形成m维的半正定矩阵，这个矩阵也被称为再生核希尔伯特空间(RKHS)，常见的核函数有：

-   线性核：$K(x_i,x_j)=x_i^Tx_j$

-   多项式核：$K(x_i,x_j)=(x_i^Tx_j)^d$

-   高斯核：$K(x_i,x_j)=\exp(-\frac{||x_i-x_j||^2}{2\sigma^2})$

-   拉普拉斯核：$K(x_i,x_j)=\exp(-\frac{||x_i-x_j||}{\sigma})$

-   Sigmod核：$K(x_i,x_j)=\tanh (\beta x_i^Tx_j+\theta)$

借助核方法和核函数我们可以把线性分类器用到非线性分类的问题上，但是具体如何选择核函数是未知的，或许需要自己一个个去尝试。


