# 02. 解耦表示学习


## 1. Dual Cross-modal Information Disentangling

- 原论文：Achieving Cross Modal Generalization with Multimodal Unified Representation, NIPS 2023

![](resources/Pasted%20image%2020231120193540.png)

文章中提到的对偶跨模态信息解耦设计目的是将两个模态A和B的信息进行解耦，A和B两个模态中同时包含了模态共通的信息和模态专门的信息，模态共通的信息代表了不同模态都可以表示的那一部分信息，而模态专门的信息代表了只有那个模态能表达的信息，这样一来两个模态就需要学习四个表示，，而这篇论文中提出的方法目的就是对每个模态的两个不同的表示向量分别进行学习。
具体来说，要让每个模态的两个向量能够表示的信息差异化，使其信息相关度最小化，因此需要做互信息最小化处理。而对于不同模态的模态共通表示，又要做互信息最大化以保证他们能够学到在模态之间共通的内容。在实现的时候，作者用Club实现了互信息最小化，用InfoNCE对比学习loss实现了互信息最大化。
Club是一个经典的互信息上界估计方法，可以通过优化下面这个目标来实现互信息的解耦：

$$
I_{\mathrm{CLUB}}(\mathbf{x} ; \mathbf{y}):=\mathbb{E}_{p(\mathbf{x}, \mathbf{y})}\left[\log q_\theta(\mathbf{y} \mid \mathbf{x})\right]-\mathbb{E}_{p(\mathbf{x})} \mathbb{E}_{p(\mathbf{y})}\left[\log q_\theta(\mathbf{y} \mid \mathbf{x})\right]
$$

这里的$q_{\theta}$是一个后验概率逼近器，在实际的实现中是一个神经网络模型(一般就2-4层)，然后在实际训练的过程中，这个神经网络模型需要和主体模型**分开训练并且轮流优化**。
Club是一个很经典的互信息最小化实现方案，在KGC领域的的DisenKGAT中也用到过，目的是对不同的解耦表示做正则化。


## 2. Mutual Information For Fair RecSys

- 原论文：Fair Representation Learning for Recommendation: A Mutual Information Perspective，AAAI 2023

![](resources/Pasted%20image%2020231205125018.png)

这篇文章主要的目标是解决推荐系统中的公平性问题，为此，作者提出的方法是将用户和商品的敏感信息(如性别、种族)的表示(s)和不敏感的信息表示(z)进行解耦，并通过优化不同embedding之间的互信息来实现这一目的。互信息优化的目标具体来说有两个方面：
- 对于每个商品或者用户，要让其两个表示s和z之间的互信息最小化，具体的实现方式就是优化其CLUB互信息上界
- 对于每个用户以及与之关联的商品，要在**给定敏感表示s的条件下最大化z和商品信息之间的互信息**，具体的实现方式就是优化其互信息下界，即使用条件InfoNCE损失来实现，这个过程的设计是这样的：

$$
\mathcal{I}\left(\mathbf{e}_u^z ; \mathbf{p}_u \mid \mathbf{e}_u^s\right) \geq \mathbb{E}\left[\log \frac{\exp f\left(\mathbf{p}_u, \mathbf{e}_u^z, \mathbf{e}_u^s\right)}{\frac{1}{M} \sum_{j=1}^M \exp f\left(\mathbf{p}_j, \mathbf{e}_u^z, \mathbf{e}_u^s\right)}\right]
$$

最终模型的互信息优化部分的损失函数就长这样：

$$
\mathcal{L}_{M I}=\beta\left(\mathcal{L}_{\text {upper }}^{\text {user }}+\mathcal{L}_{\text {upper }}^{\text {item }}\right)-\gamma\left(\mathcal{L}_{\text {lower }}^{\text {user }}+\mathcal{L}_{\text {lower }}^{\text {item }}\right)
$$

这篇论文讲故事的关键点有两个，一是为什么要最大化/最小化某部分的互信息，这部分motivation其实还算比较容易想到，第二个很关键的地方其实是为什么要在给定敏感表示s的条件下最大化z和商品信息之间的互信息，一般来说互信息优化用的都是InfoNCE，为什么这里要用条件InfoNCE呢？
作者在文中的解释是，如果不这么做，那么这个不敏感信息表示z中包含的信息实际上就是从biased的用户历史记录中提取出来的同时包含敏感信息和不敏感信息的决策结果，这样没办法实现两个表示的解耦，所以要在给定敏感信息表示s的情况下去最大化不敏感信息表示z和用户历史记录之间的互信息，这样一来就可以实现敏感信息和不敏感信息的解耦，具体的做法就是在相似度函数f计算的过程中将两个表示加权求和，这样一来两种表示就被人为地分开定义了出来。